{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI-Compatible Batch Inference Demo\n",
    "\n",
    "This notebook demonstrates how to use the batch inference API with OpenAI-compatible endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c290db36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "from IPython.display import JSON, display\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Configuration\n",
    "BASE_URL = \"http://localhost:8000\"  # Change if your server runs elsewhere"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4544da8",
   "metadata": {},
   "source": [
    "## 2. Load Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ea226b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample prompts from JSONL file\n",
    "with open('sample_batch.jsonl', 'r') as f:\n",
    "    prompts = [json.loads(line)['prompt'] for line in f if line.strip()]\n",
    "\n",
    "logger.info(f\"Loaded {len(prompts)} sample prompts:\")\n",
    "for i, prompt in enumerate(prompts, 1):\n",
    "    logger.info(f\"{i}. {prompt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e42472",
   "metadata": {},
   "source": [
    "## 3. Submit Batch Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eac5946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit batch job\n",
    "batch_request = {\n",
    "    \"model\": \"Qwen/Qwen2.5-0.5B-Instruct\",\n",
         "    \"input\": [{\"prompt\": m} for m in prompts],\n",
    "    \"max_tokens\": 100,\n",
    "    \"temperature\": 0.7\n",
    "}\n",
    "\n",
    "logger.info(\"Submitting batch job...\")\n",
    "response = requests.post(\n",
    "    f\"{BASE_URL}/v1/batches\",\n",
    "    json=batch_request\n",
    ")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    batch_id = data[\"id\"]\n",
    "    logger.info(f\"Batch created with ID: {batch_id}\")\n",
    "    logger.info(f\"Status: {data['status']}\")\n",
    "    logger.info(f\"Created at: {data['created_at']}\")\n",
    "else:\n",
    "    logger.error(f\"Failed to create batch: {response.status_code}\")\n",
    "    logger.error(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7d28c9",
   "metadata": {},
   "source": [
    "## 4. Monitor Job Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cbf91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check job status\n",
    "def check_job_status(batch_id):\n",
    "    response = requests.get(f\"{BASE_URL}/v1/batches/{batch_id}\")\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        return data\n",
    "    else:\n",
    "        logger.error(f\"Failed to get status: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "# Monitor job progress\n",
    "logger.info(\"Monitoring job progress...\")\n",
    "for i in range(10):  # Check 10 times with 2-second intervals\n",
    "    status_data = check_job_status(batch_id)\n",
    "    if status_data:\n",
    "        status = status_data[\"status\"]\n",
    "        logger.info(f\"Check {i+1}: {status}\")\n",
    "        if status in [\"completed\", \"failed\"]:\n",
    "            break\n",
    "    time.sleep(2.0)\n",
    "else:\n",
    "    logger.warning(\"Timeout or error checking status\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7b4a77",
   "metadata": {},
   "source": [
    "## 5. Retrieve Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba4f13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get final results\n",
    "logger.info(\"Retrieving final results...\")\n",
    "response = requests.get(f\"{BASE_URL}/v1/batches/{batch_id}/results\")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    results = data.get(\"data\", [])\n",
    "    logger.info(f\"Retrieved {len(results)} results:\")\n",
    "    display(JSON(results))\n",
    "    \n",
    "    # Show first few results\n",
    "    for i, result in enumerate(results[:3], 1):\n",
    "        prompt = result.get(\"prompt\", \"\")\n",
    "        response_text = result.get(\"response\", \"\")\n",
    "        tokens = result.get(\"tokens\", 0)\n",
    "        logger.info(f\"--- Result {i} ---\")\n",
    "        logger.info(f\"Prompt: {prompt}\")\n",
    "        logger.info(f\"Response: {response_text}\")\n",
    "        logger.info(f\"Tokens: {tokens}\")\n",
    "else:\n",
    "    logger.error(f\"Failed to get results: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff06f85",
   "metadata": {},
   "source": [
    "## 6. Summary\n",
    "\n",
    "This notebook demonstrated the complete OpenAI-compatible batch inference workflow:\n",
    "\n",
    "1. **Batch Creation** - Submit prompts via POST `/v1/batches`\n",
    "2. **Job Processing** - Background worker processes jobs asynchronously\n",
    "3. **Status Monitoring** - Poll job status until completion\n",
    "4. **Result Retrieval** - Get processed results via `/v1/batches/{id}/results`\n",
    "\n",
    "The system uses:\n",
    "- **File-based job storage** (JSON + JSONL format)\n",
    "- **In-memory queue** for job scheduling\n",
    "- **Background worker** for asynchronous processing\n",
    "- **Mock inference engine** for demonstration purposes\n",
    "\n",
    "**Production upgrades** would include:\n",
    "- Real Ray Data + vLLM integration\n",
    "- Redis/RabbitMQ for job queuing\n",
    "- Database for job metadata\n",
    "- SLA-aware scheduling\n",
    "- Docker/Kubernetes deployment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
