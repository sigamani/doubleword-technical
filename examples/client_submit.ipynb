{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b794c81",
   "metadata": {},
   "source": [
    "# OpenAI-Compatible Batch Inference Demo\n",
    "\n",
    "This notebook demonstrates how to use the batch inference API with OpenAI-compatible endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c76bbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests\n",
    "!pip install -r ../requirements.dev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77870f6a",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c290db36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "from IPython.display import JSON, display\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "BASE_URL = \"http://localhost:8000\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4544da8",
   "metadata": {},
   "source": [
    "## 2. Start API Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87adaea4-f256-4cbd-9aa2-16bdd7e1eaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import time\n",
    "import threading\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Start server in background\n",
    "def start_api_server():\n",
    "    env = os.environ.copy()\n",
    "    env[\"ENVIRONMENT\"] = \"DEV\"\n",
    "    env[\"GPU_AVAILABLE\"] = \"false\"\n",
    "    \n",
    "    cmd = [\"python\", \"api/main.py\"]\n",
    "    \n",
    "    return subprocess.Popen(\n",
    "        cmd,\n",
    "        cwd=\"/Users/michaelsigamani/Documents/DevelopmentCode/2025-fall/PoC-offline-batch-inference\",\n",
    "        env=env,\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.PIPE\n",
    "    )\n",
    "# Start the server\n",
    "print(\"Starting API server...\")\n",
    "server_process = start_api_server()\n",
    "time.sleep(3)  # Wait for server to start\n",
    "display(Markdown(\"Server started on http://localhost:8000\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4544da8",
   "metadata": {},
   "source": [
    "## 3. Load Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ea226b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sample_batch.jsonl', 'r') as f:\n",
    "    prompts = [json.loads(line)['prompt'] for line in f if line.strip()]\n",
    "\n",
    "logger.info(f\"Loaded {len(prompts)} sample prompts:\")\n",
    "for i, prompt in enumerate(prompts, 1):\n",
    "    logger.info(f\"{i}. {prompt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e42472",
   "metadata": {},
   "source": [
    "## 4. Submit Initial Batch Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eac5946",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_request = {\n",
    "    \"model\": \"Qwen/Qwen2.5-0.5B-Instruct\",\n",
    "    \"input\": [{\"prompt\": m} for m in prompts],\n",
    "    \"max_tokens\": 100,\n",
    "    \"temperature\": 0.7\n",
    "}\n",
    "\n",
    "logger.info(\"Submitting batch job...\")\n",
    "response = requests.post(\n",
    "    f\"{BASE_URL}/v1/batches\",\n",
    "    json=batch_request\n",
    ")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    batch_id = data[\"id\"]\n",
    "    logger.info(f\"Batch created with ID: {batch_id}\")\n",
    "    logger.info(f\"Status: {data['status']}\")\n",
    "    logger.info(f\"Created at: {data['created_at']}\")\n",
    "else:\n",
    "    logger.error(f\"Failed to create batch: {response.status_code}\")\n",
    "    logger.error(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7d28c9",
   "metadata": {},
   "source": [
    "## 5. Monitor Job Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cbf91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_job_status(batch_id):\n",
    "    response = requests.get(f\"{BASE_URL}/v1/batches/{batch_id}\")\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        return data\n",
    "    else:\n",
    "        logger.error(f\"Failed to get status: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "logger.info(\"Monitoring job progress...\")\n",
    "for i in range(10):  \n",
    "    status_data = check_job_status(batch_id)\n",
    "    if status_data:\n",
    "        status = status_data[\"status\"]\n",
    "        logger.info(f\"Check {i+1}: {status}\")\n",
    "        if status in [\"completed\", \"failed\"]:\n",
    "            break\n",
    "    time.sleep(2.0)\n",
    "else:\n",
    "    logger.warning(\"Timeout or error checking status\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7b4a77",
   "metadata": {},
   "source": [
    "## 6. Retrieve Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba4f13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Retrieving final results...\")\n",
    "response = requests.get(f\"{BASE_URL}/v1/batches/{batch_id}/results\")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    results = data.get(\"data\", [])\n",
    "    logger.info(f\"Retrieved {len(results)} results:\")\n",
    "    display(JSON(results))\n",
    "    \n",
    "    for i, result in enumerate(results[:3], 1):\n",
    "        prompt = result.get(\"prompt\", \"\")\n",
    "        response_text = result.get(\"response\", \"\")\n",
    "        tokens = result.get(\"tokens\", 0)\n",
    "        logger.info(f\"--- Result {i} ---\")\n",
    "        logger.info(f\"Prompt: {prompt}\")\n",
    "        logger.info(f\"Response: {response_text}\")\n",
    "        logger.info(f\"Tokens: {tokens}\")\n",
    "else:\n",
    "    logger.error(f\"Failed to get results: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff06f85",
   "metadata": {},
   "source": [
    "## 7. Debug Queue Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d77746-a3a1-4318-8438-42d93506fbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check job status to see if it is still processing\n",
    "response = requests.get(f\"{BASE_URL}/v1/batches/{batch_id}\")\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    print(f\"Job ID: {data['id']}\")\n",
    "    print(f\"Status: {data['status']}\")\n",
    "    print(f\"Created at: {data['created_at']}\")\n",
    "    print(f\"Completed at: {data.get('completed_at', 'Not completed yet')}\")\n",
    "else:\n",
    "    print(f\"Failed to get status: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f66c191-518b-44ab-babe-9623797751f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check debug endpoints to see what is happening\n",
    "response = requests.get(f\"{BASE_URL}/debug/worker\")\n",
    "if response.status_code == 200:\n",
    "    debug_info = response.json()\n",
    "    print(f\"Worker running: {debug_info['worker_running']}\")\n",
    "    print(f\"Queue depth: {debug_info['queue_depth']}\")\n",
    "    print(f\"Worker thread alive: {debug_info['worker_thread_alive']}\")\n",
    "else:\n",
    "    print(\"Debug endpoint not available\")\n",
    "# Check GPU pool status\n",
    "response = requests.get(f\"{BASE_URL}/debug/gpu-pools\")\n",
    "if response.status_code == 200:\n",
    "    pool_info = response.json()\n",
    "    print(f\"GPU pools: {pool_info}\")\n",
    "else:\n",
    "    print(\"GPU debug endpoint not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d482f79d-3786-40c2-bd8a-3c16b336fda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "time.sleep(5)\n",
    "response = requests.get(f\"{BASE_URL}/debug/worker\")\n",
    "if response.status_code == 200:\n",
    "    debug_info = response.json()\n",
    "    print(f\"Queue depth after 5 seconds: {debug_info['queue_depth']}\")\n",
    "    print(f\"Worker running: {debug_info['worker_running']}\")\n",
    "    print(f\"Worker thread alive: {debug_info['worker_thread_alive']}\")\n",
    "else:\n",
    "    print(\"Debug endpoint not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f66c191-518b-44ab-babe-9623797751f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check queue state before submission\n",
    "response = requests.get(f\"{BASE_URL}/debug/worker\")\n",
    "if response.status_code == 200:\n",
    "    debug_info = response.json()\n",
    "    print(f\"Queue depth BEFORE: {debug_info['queue_depth']}\")\n",
    "\n",
    "# Submit a new job\n",
    "batch_request = {\n",
    "    \"model\": \"Qwen/Qwen2.5-0.5B-Instruct\", \n",
    "    \"input\": [{\"prompt\": \"Debug test prompt\"}],\n",
    "    \"max_tokens\": 50,\n",
    "    \"temperature\": 0.7\n",
    "}\n",
    "\n",
    "print(\"Submitting new batch...\")\n",
    "response = requests.post(f\"{BASE_URL}/v1/batches\", json=batch_request)\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    batch_id = data[\"id\"]\n",
    "    print(f\"Submitted batch ID: {batch_id}\")\n",
    "else:\n",
    "    print(f\"Failed to submit: {response.status_code}\")\n",
    "    print(response.text)\n",
    "\n",
    "# Check queue depth immediately after submission\n",
    "response = requests.get(f\"{BASE_URL}/debug/worker\")\n",
    "if response.status_code == 200:\n",
    "    debug_info = response.json()\n",
    "    print(f\"Queue depth AFTER: {debug_info['queue_depth']}\")\n",
    "\n",
    "# Check if job file was created\n",
    "import os\n",
    "import json\n",
    "\n",
    "if 'batch_id' in locals():\n",
    "    job_file = f\"/tmp/job_{batch_id}.json\"\n",
    "    print(f\"Job file exists: {os.path.exists(job_file)}\")\n",
    "    \n",
    "    if os.path.exists(job_file):\n",
    "        with open(job_file, 'r') as f:\n",
    "            job_data = json.load(f)\n",
    "        print(f\"Job file status: {job_data.get('status', 'No status')}\")\n",
    "        print(f\"Job file created at: {job_data.get('created_at', 'No timestamp')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f66c191-518b-44ab-babe-9623797751f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "time.sleep(5)\n",
    "response = requests.get(f\"{BASE_URL}/debug/worker\")\n",
    "if response.status_code == 200:\n",
    "    debug_info = response.json()\n",
    "    print(f\"Queue depth after 5 seconds: {debug_info['queue_depth']}\")\n",
    "    print(f\"Worker running: {debug_info['worker_running']}\")\n",
    "    print(f\"Worker thread alive: {debug_info['worker_thread_alive']}\")\n",
    "\n",
    "response = requests.get(f\"{BASE_URL}/debug/queue-contents\")\n",
    "if response.status_code == 200:\n",
    "    queue_data = response.json()\n",
    "    print(f\"Queue contents: {queue_data}\")\n",
    "else:\n",
    "    print(\"Queue contents endpoint not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f66c191-518b-44ab-babe-9623797751f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch_request = {\n",
    "    \"model\": \"Qwen/Qwen2.5-0.5B-Instruct\",\n",
    "    \"input\": [{\"prompt\": \"Test after queueing fix\"}],\n",
    "    \"max_tokens\": 50,\n",
    "    \"temperature\": 0.7\n",
    "}\n",
    "\n",
    "print(\"\\n== Testing Fixed Queueing ===\")\n",
    "response = requests.post(f\"{BASE_URL}/v1/batches\", json=test_batch_request)\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    test_batch_id = data[\"id\"]\n",
    "    print(f\"Test batch created: {test_batch_id}\")\n",
    "    print(f\"Status: {data['status']}\")\n",
    "else:\n",
    "    print(f\"Failed to create test batch: {response.status_code}\")\n",
    "    print(response.text)\n",
    "\n",
    "# Wait a bit and check if job processes\n",
    "time.sleep(10)  # Give time for processing\n",
    "\n",
    "# Check final status\n",
    "response = requests.get(f\"{BASE_URL}/v1/batches/{test_batch_id}\")\n",
    "if response.status_code == 200:\n",
    "    status_data = response.json()\n",
    "    final_status = status_data[\"status\"]\n",
    "    print(f\"Final status: {final_status}\")\n",
    "    \n",
    "    if final_status == \"completed\":\n",
    "        # Try to get results\n",
    "        response = requests.get(f\"{BASE_URL}/v1/batches/{test_batch_id}/results\")\n",
    "        if response.status_code == 200:\n",
    "            results_data = response.json()\n",
    "            results = results_data.get(\"data\", [])\n",
    "            print(f\" Successfully retrieved {len(results)} results!\")\n",
    "            for i, result in enumerate(results[:2], 1):\n",
    "                prompt = result.get(\"prompt\", \"\")\n",
    "                response_text = result.get(\"response\", \"\")\n",
    "                tokens = result.get(\"tokens\", 0)\n",
    "                print(f\"Result {i}: {response_text[:50]}...\")\n",
    "        else:\n",
    "            print(f\"Results endpoint returned: {response.status_code}\")\n",
    "    else:\n",
    "        print(f\"Job ended with status: {final_status}\")\n",
    "else:\n",
    "    print(f\"Failed to check final status: {response.status_code}\")\n",
    "\n",
    "print(\"\\n=== Test Complete ===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff06f85",
   "metadata": {},
   "source": [
    "## 10. Summary\n",
    "\n",
    "This notebook demonstrates the complete OpenAI-compatible batch inference workflow:\n",
    "\n",
    "### **Setup Steps:**\n",
    "1. **Start API Server** - Launch FastAPI server in DEV mode\n",
    "2. **Load Sample Data** - Read test prompts from JSONL file\n",
    "3. **Submit Batch Job** - Create batch via POST request\n",
    "4. **Monitor Progress** - Poll job status until completion\n",
    "5. **Retrieve Results** - Get processed results when complete\n",
    "\n",
    "### **Debugging Tools:**\n",
    "- **Worker Debug** - Check `/debug/worker` for queue depth and worker status\n",
    "- **GPU Pool Debug** - Check `/debug/gpu-pools` for resource allocation\n",
    "- **Queue Contents** - Check `/debug/queue-contents` to see actual queue state\n",
    "- **Job Status** - Direct status checks for individual batches\n",
    "\n",
    "### **PoC Features:**\n",
    "- **Thread-safe operations** - Concurrent request handling\n",
    "- **GPU resource management** - Spot/dedicated pool allocation\n",
    "- **SLA-aware scheduling** - Priority-based job processing\n",
    "- **Error handling** - Proper retries and fallback behavior\n",
    "- **OpenAI compatibility** - Standard batch API endpoints"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (PoC Batch Inference)",
   "language": "python",
   "name": "poc-batch-inference"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
