# Docker Compose configuration for the API and vLLM services. This file defines and runs a
# multi-container setup intended for a staging environment. The goal is to simulate the
# production architecture as closely as possible. In practice, this setup is designed to run
# against spot and dedicated GPU instances rented from faster, lower-cost providers such as
# Vast.ai or Lambda Labs before deploying to AWS, GCP, or Azure.

services:
  api:
    build:
      context: ..
      dockerfile: docker/Dockerfile.api
    environment:
      - RAY_BACKEND=log
      - ENVIRONMENT=STAGE
      - GPU_AVAILABLE=true
    working_dir: /app
    command: uvicorn api.main:app --host 0.0.0.0 --port 8000
    ports:
      - "8000:8000"
    volumes:
        - ../api:/app/api
        - ../pipeline:/app/pipeline
        - batch_storage:/tmp
  vllm:
    build:
      context: ..
      dockerfile: docker/Dockerfile.vllm
    environment:
      - RAY_BACKEND=log
      - ENVIRONMENT=STAGE
      - GPU_AVAILABLE=true 
    working_dir: /app
    
    ports:
      - "8001:8001"
    volumes:
        - ../api:/app/api
        - ../pipeline:/app/pipeline
        - batch_storage:/tmp
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
volumes:
  batch_storage:
    driver: local
