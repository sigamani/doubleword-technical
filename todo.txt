# Batch Inference Testing Plan: 2-Node Setup with Qwen2.5-0.5B

## Overview

I'm testing a scalable batch inference architecture using a small model (Qwen2.5-0.5B) on 2 nodes before scaling to production. The goal is to validate the architecture, observability, and configurability to meet a 24-hour SLA.

Test Strategy: Start small with 2x 3090 GPUs and a 0.5B model, validate the full stack (data pipeline, inference, monitoring, SLA tracking), then scale up to larger models and more nodes.

---

## Implementation

### 1. Dependencies

pip install ray[data] vllm datasets pyyaml


### 2. Configuration File (config.yaml)

Make all parameters tunable via YAML:

model:
  name: "Qwen/Qwen2.5-0.5B-Instruct"
  max_model_len: 32768
  tensor_parallel_size: 1

inference:
  batch_size: 128
  concurrency: 2
  max_num_batched_tokens: 16384
  gpu_memory_utilization: 0.90
  temperature: 0.7
  max_tokens: 512

data:
  input_path: "s3://bucket/sharegpt_sample.json"
  output_path: "s3://bucket/outputs/"
  num_samples: 1000

sla:
  target_hours: 24
  buffer_factor: 0.7  # 30% safety margin
  alert_threshold_hours: 20


### 3. Load Test Data (ShareGPT)

import ray
from datasets import load_dataset

ray.init(address="auto")

# Load ShareGPT dataset
hf_ds = load_dataset("anon8231489123/ShareGPT_Vicuna_unfiltered", split="train[:1000]")
ds = ray.data.from_huggingface(hf_ds)

# Extract prompts from ShareGPT format
def format_sharegpt(row):
    messages = row["conversations"]
    prompt = next((m["value"] for m in messages if m["from"] == "human"), "")
    return {"prompt": prompt}

ds = ds.map(format_sharegpt)


### 4. Configure vLLM Processor

from ray.data.llm import vLLMEngineProcessorConfig, build_llm_processor
import yaml

# Load config
with open("config.yaml") as f:
    config = yaml.safe_load(f)

# Setup vLLM engine
vllm_config = vLLMEngineProcessorConfig(
    model=config["model"]["name"],
    concurrency=config["inference"]["concurrency"],
    batch_size=config["inference"]["batch_size"],
    engine_kwargs={
        "max_num_batched_tokens": config["inference"]["max_num_batched_tokens"],
        "max_model_len": config["model"]["max_model_len"],
        "gpu_memory_utilization": config["inference"]["gpu_memory_utilization"],
        "tensor_parallel_size": config["model"]["tensor_parallel_size"],
        "enable_chunked_prefill": True,
    }
)


### 5. Observability & Monitoring

import time
import logging
from dataclasses import dataclass

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class BatchMetrics:
    total_requests: int
    completed_requests: int = 0
    failed_requests: int = 0
    start_time: float = None
    tokens_processed: int = 0
    
    def __post_init__(self):
        if self.start_time is None:
            self.start_time = time.time()
    
    def throughput_per_sec(self) -> float:
        elapsed = time.time() - self.start_time
        return self.completed_requests / elapsed if elapsed > 0 else 0
    
    def tokens_per_sec(self) -> float:
        elapsed = time.time() - self.start_time
        return self.tokens_processed / elapsed if elapsed > 0 else 0
    
    def eta_hours(self) -> float:
        throughput = self.throughput_per_sec()
        if throughput == 0:
            return float('inf')
        remaining = self.total_requests - self.completed_requests
        return (remaining / throughput) / 3600
    
    def progress_pct(self) -> float:
        return (self.completed_requests / self.total_requests) * 100

class InferenceMonitor:
    def __init__(self, metrics: BatchMetrics, sla_hours: float, log_interval: int = 100):
        self.metrics = metrics
        self.sla_hours = sla_hours
        self.log_interval = log_interval
        self.last_log = 0
    
    def update(self, batch_size: int, tokens: int):
        self.metrics.completed_requests += batch_size
        self.metrics.tokens_processed += tokens
        
        if self.metrics.completed_requests - self.last_log >= self.log_interval:
            self.log_progress()
            self.check_sla()
            self.last_log = self.metrics.completed_requests
    
    def log_progress(self):
        logger.info(
            f"Progress: {self.metrics.progress_pct():.1f}% | "
            f"Completed: {self.metrics.completed_requests}/{self.metrics.total_requests} | "
            f"Throughput: {self.metrics.throughput_per_sec():.2f} req/s | "
            f"Tokens/sec: {self.metrics.tokens_per_sec():.2f} | "
            f"ETA: {self.metrics.eta_hours():.2f}h | "
            f"Failed: {self.metrics.failed_requests}"
        )
    
    def check_sla(self) -> bool:
        eta = self.metrics.eta_hours()
        elapsed_hours = (time.time() - self.metrics.start_time) / 3600
        remaining_hours = self.sla_hours - elapsed_hours
        
        if eta > remaining_hours:
            logger.warning(f"SLA AT RISK! ETA {eta:.2f}h > Remaining {remaining_hours:.2f}h")
            return False
        return True

# Initialize monitoring
metrics = BatchMetrics(total_requests=len(ds))
monitor = InferenceMonitor(metrics, sla_hours=config["sla"]["target_hours"])


### 6. Run Inference with Monitoring

def preprocess(row):
    return {
        "messages": [
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": row["prompt"]}
        ],
        "sampling_params": {
            "temperature": config["inference"]["temperature"],
            "max_tokens": config["inference"]["max_tokens"],
        }
    }

def postprocess(row):
    # Estimate tokens (rough approximation)
    tokens = len(row["generated_text"].split()) * 1.3
    monitor.update(batch_size=1, tokens=int(tokens))
    
    return {
        "response": row["generated_text"],
        "prompt": row.get("prompt", ""),
    }

# Build processor
processor = build_llm_processor(vllm_config, preprocess=preprocess, postprocess=postprocess)

# Run inference
logger.info("Starting batch inference...")
result_ds = processor(ds)

# Write results incrementally
result_ds.write_parquet(config["data"]["output_path"], try_create_dir=True)

# Final report
monitor.log_progress()
logger.info(f"Batch complete! Total time: {(time.time() - metrics.start_time)/3600:.2f}h")


---

## What to Monitor

### Key Performance Indicators (KPIs)

1. Throughput Metrics
   - Requests per second
   - Tokens per second
   - Completion percentage

2. SLA Tracking
   - ETA vs. remaining time
   - On-track status (alert if ETA > remaining budget)

3. Resource Utilization
   - GPU utilization (via Ray dashboard or nvidia-smi)
   - GPU memory usage
   - CPU utilization
   - Network I/O (for S3 writes)

4. Quality Metrics
   - Failure rate
   - Average response length
   - Error patterns

### Monitoring Commands

# Check GPU utilization
watch -n 1 nvidia-smi

# Monitor Ray dashboard
# Access at http://localhost:8265

# Track output files
aws s3 ls s3://bucket/outputs/ --recursive --human-readable


---

## What to Configure

### Parameters to Tune for Performance

1. Batch Size (inference.batch_size)
   - Start: 128 for 0.5B model
   - Increase for higher throughput
   - Decrease if OOM errors occur

2. Concurrency (inference.concurrency)
   - Number of parallel workers
   - Set to number of GPUs for independent models
   - Experiment with higher values

3. GPU Memory Utilization (inference.gpu_memory_utilization)
   - Start: 0.90
   - Reduce if experiencing OOM
   - Increase for maximum throughput

4. Max Model Length (model.max_model_len)
   - Limits context window
   - Reduce to fit larger batches in memory

5. Max Num Batched Tokens (inference.max_num_batched_tokens)
   - Total tokens processed in a batch
   - Balance between throughput and memory

### Testing Matrix

Test these configurations systematically:

Model Size    Batch Size    Concurrency    Expected Impact
0.5B          128           2              Baseline
0.5B          256           2              +30-50% throughput
0.5B          128           4              Test scaling
7B            64            2              Validate larger model
13B           32            2              Test memory limits


### Auto-Tuning Script

def benchmark_config(test_config, sample_size=100):
    """Test a configuration on a small sample"""
    test_ds = ds.limit(sample_size)
    
    start = time.time()
    test_processor = build_llm_processor(test_config, preprocess, postprocess)
    results = test_processor(test_ds).take_all()
    elapsed = time.time() - start
    
    throughput = sample_size / elapsed
    return {
        "throughput_req_per_sec": throughput,
        "estimated_24h_capacity": throughput * 86400,
        "config": test_config
    }

# Test multiple configurations
configs_to_test = [
    {"batch_size": 64, "concurrency": 2},
    {"batch_size": 128, "concurrency": 2},
    {"batch_size": 256, "concurrency": 2},
]

for cfg in configs_to_test:
    result = benchmark_config(cfg)
    print(f"Config: {cfg} -> Throughput: {result['throughput_req_per_sec']:.2f} req/s")


---

## Expected Outputs

1. Real-time logs showing progress every 100 requests
2. SLA warnings if falling behind schedule
3. Parquet files written incrementally to S3
4. Final performance report with total time and throughput
5. KPI metrics exportable to monitoring dashboards

---

## Next Steps

1. Run with 1,000 samples to validate pipeline
2. Scale to 10,000+ samples to test SLA tracking
3. Test with 7B model to validate memory limits
4. Add Prometheus/Grafana for production monitoring
5. Implement automated alerting for SLA violations
